---
prometheus-operator-crds:
  crds:
    annotations:
      argocd.argoproj.io/sync-options: ServerSideApply=true

prometheus:
  alertmanager:
    enabled: true
    # overriding to main until a release > 0.27 is created for alert manager:
    # https://github.com/prometheus/alertmanager
    image:
      tag: "main"

    podAnnotations:
      diun.enable: "false"
      vault.hashicorp.com/agent-inject: "true"
      vault.hashicorp.com/role: internal-app
      vault.hashicorp.com/agent-inject-template-discord_url: |
        {{- with secret "kv/data/notifications" -}}
        {{- .Data.data.DISCORD_ALERT -}}
        {{- end }}

    config:
      route:
        group_by: ['alertname', 'job']
        group_wait: 15s
        group_interval: 15s
        repeat_interval: 15s
        receiver: discord

        routes:
          # continue defaults to false, so the first match will end routing.
          - match:
              # This was previously named DeadMansSwitch
              alertname: Watchdog
            receiver: "null"
          - receiver: discord
            group_wait: 10s
            match:
              severity: warning
          - receiver: discord
            group_wait: 10s
            match:
              severity: warning

      receivers:
        - name: "null"
        - name: discord
          # disable discord until fix gets merged
          discord_configs:
            - webhook_url_file: '/vault/secrets/discord_url'
    
  serverFiles:
    alerting_rules.yml:
      groups:
        # https://samber.github.io/awesome-prometheus-alerts/rules.html
        - name: Prometheus self-monitoring
          rules:
            - alert: PrometheusTooManyRestarts
              expr: changes(process_start_time_seconds{job=~"prometheus|pushgateway|alertmanager"}[15m]) > 2
              for: 0m
              labels:
                severity: warning
              annotations:
                summary: Prometheus too many restarts (instance {{ $labels.instance }})
                description: "Prometheus has restarted more than twice in the last 15 minutes. It might be crashlooping.\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"
            - alert: Watchdog
              expr: vector(1)
              for: 0m
              labels:
                severity: critical
              annotations:
                summary: Prometheus AlertManager E2E dead man switch (instance {{ $labels.instance }})
                description: "Prometheus DeadManSwitch is an always-firing alert. It's used as an end-to-end test of Prometheus through the Alertmanager.\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"
            - alert: PrometheusRuleEvaluationFailures
              expr: increase(prometheus_rule_evaluation_failures_total[3m]) > 0
              for: 0m
              labels:
                severity: critical
              annotations:
                summary: Prometheus rule evaluation failures (instance {{ $labels.instance }})
                description: "Prometheus encountered {{ $value }} rule evaluation failures, leading to potentially ignored alerts.\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"
            - alert: PrometheusAlertmanagerNotificationFailing
              expr: rate(alertmanager_notifications_failed_total[1m]) > 0
              for: 0m
              labels:
                severity: critical
              annotations:
                summary: Prometheus AlertManager notification failing (instance {{ $labels.instance }})
                description: "Alertmanager is failing sending notifications\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"
            - alert: PrometheusTargetScrapingSlow
              expr: prometheus_target_interval_length_seconds{quantile="0.9"} / on (interval, instance, job) prometheus_target_interval_length_seconds{quantile="0.5"} > 1.05
              for: 5m
              labels:
                severity: warning
              annotations:
                summary: Prometheus target scraping slow (instance {{ $labels.instance }})
                description: "Prometheus is scraping exporters slowly since it exceeded the requested interval time. Your Prometheus server is under-provisioned.\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"
            - alert: PrometheusLargeScrape
              expr: increase(prometheus_target_scrapes_exceeded_sample_limit_total[10m]) > 10
              for: 5m
              labels:
                severity: warning
              annotations:
                summary: Prometheus large scrape (instance {{ $labels.instance }})
                description: "Prometheus has many scrapes that exceed the sample limit\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

        - name: Host and Hardware
          rules:
            - alert: HostOutOfMemory
              expr: (node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes * 100 < 10) * on(instance) group_left (nodename) node_uname_info{nodename=~".+"}
              for: 2m
              labels:
                severity: warning
              annotations:
                summary: Host out of memory (instance {{ $labels.instance }})
                description: "Node memory is filling up (< 10% left)\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"
            - alert: HostMemoryUnderMemoryPressure
              expr: (rate(node_vmstat_pgmajfault[1m]) > 1000) * on(instance) group_left (nodename) node_uname_info{nodename=~".+"}
              for: 2m
              labels:
                severity: warning
              annotations:
                summary: Host memory under memory pressure (instance {{ $labels.instance }})
                description: "The node is under heavy memory pressure. High rate of major page faults\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"
            # You may want to increase the alert manager 'repeat_interval' for this type of alert to daily or weekly
            - alert: HostMemoryIsUnderutilized
              expr: (100 - (avg_over_time(node_memory_MemAvailable_bytes[30m]) / node_memory_MemTotal_bytes * 100) < 20) * on(instance) group_left (nodename) node_uname_info{nodename=~".+"}
              for: 1w
              labels:
                severity: info
              annotations:
                summary: Host Memory is underutilized (instance {{ $labels.instance }})
                description: "Node memory is < 20% for 1 week. Consider reducing memory space. (instance {{ $labels.instance }})\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"
            - alert: HostUnusualDiskReadRate
              expr: (sum by (instance) (rate(node_disk_read_bytes_total[2m])) / 1024 / 1024 > 50) * on(instance) group_left (nodename) node_uname_info{nodename=~".+"}
              for: 5m
              labels:
                severity: warning
              annotations:
                summary: Host unusual disk read rate (instance {{ $labels.instance }})
                description: "Disk is probably reading too much data (> 50 MB/s)\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"
            - alert: HostUnusualDiskWriteRate
              expr: (sum by (instance) (rate(node_disk_written_bytes_total[2m])) / 1024 / 1024 > 50) * on(instance) group_left (nodename) node_uname_info{nodename=~".+"}
              for: 2m
              labels:
                severity: warning
              annotations:
                summary: Host unusual disk write rate (instance {{ $labels.instance }})
                description: "Disk is probably writing too much data (> 50 MB/s)\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"
            # Please add ignored mountpoints in node_exporter parameters like
            # "--collector.filesystem.ignored-mount-points=^/(sys|proc|dev|run)($|/)".
            # Same rule using "node_filesystem_free_bytes" will fire when disk fills for non-root users.
            - alert: HostOutOfDiskSpace
              expr: ((node_filesystem_avail_bytes * 100) / node_filesystem_size_bytes < 10 and ON (instance, device, mountpoint) node_filesystem_readonly == 0) * on(instance) group_left (nodename) node_uname_info{nodename=~".+"}
              for: 2m
              labels:
                severity: warning
              annotations:
                summary: Host out of disk space (instance {{ $labels.instance }})
                description: "Disk is almost full (< 10% left)\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"
            # Please add ignored mountpoints in node_exporter parameters like
            # "--collector.filesystem.ignored-mount-points=^/(sys|proc|dev|run)($|/)".
            # Same rule using "node_filesystem_free_bytes" will fire when disk fills for non-root users.
            - alert: HostDiskWillFillIn24Hours
              expr: ((node_filesystem_avail_bytes * 100) / node_filesystem_size_bytes < 10 and ON (instance, device, mountpoint) predict_linear(node_filesystem_avail_bytes{fstype!~"tmpfs"}[1h], 24 * 3600) < 0 and ON (instance, device, mountpoint) node_filesystem_readonly == 0) * on(instance) group_left (nodename) node_uname_info{nodename=~".+"}
              for: 2m
              labels:
                severity: warning
              annotations:
                summary: Host disk will fill in 24 hours (instance {{ $labels.instance }})
                description: "Filesystem is predicted to run out of space within the next 24 hours at current write rate\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"
            - alert: HostHighCpuLoad
              expr: (sum by (instance) (avg by (mode, instance) (rate(node_cpu_seconds_total{mode!="idle"}[2m]))) > 0.8) * on(instance) group_left (nodename) node_uname_info{nodename=~".+"}
              for: 10m
              labels:
                severity: warning
              annotations:
                summary: Host high CPU load (instance {{ $labels.instance }})
                description: "CPU load is > 80%\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"
            - alert: HostSwapIsFillingUp
              expr: ((1 - (node_memory_SwapFree_bytes / node_memory_SwapTotal_bytes)) * 100 > 80) * on(instance) group_left (nodename) node_uname_info{nodename=~".+"}
              for: 2m
              labels:
                severity: warning
              annotations:
                summary: Host swap is filling up (instance {{ $labels.instance }})
                description: "Swap is filling up (>80%)\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"
            - alert: HostOomKillDetected
              expr: (increase(node_vmstat_oom_kill[1m]) > 0) * on(instance) group_left (nodename) node_uname_info{nodename=~".+"}
              for: 0m
              labels:
                severity: warning
              annotations:
                summary: Host OOM kill detected (instance {{ $labels.instance }})
                description: "OOM kill detected\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

        - name: Postgresql
          rules:
            - alert: PostgresqlDown
              expr: pg_up == 0
              for: 0m
              labels:
                severity: critical
              annotations:
                summary: Postgresql down (instance {{ $labels.instance }})
                description: "Postgresql instance is down\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"
            - alert: PostgresqlRestarted
              expr: time() - pg_postmaster_start_time_seconds < 60
              for: 0m
              labels:
                severity: critical
              annotations:
                summary: Postgresql restarted (instance {{ $labels.instance }})
                description: "Postgresql restarted\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"
            - alert: PostgresqlDeadLocks
              expr: increase(pg_stat_database_deadlocks{datname!~"template.*|postgres"}[1m]) > 5
              for: 0m
              labels:
                severity: warning
              annotations:
                summary: Postgresql dead locks (instance {{ $labels.instance }})
                description: "PostgreSQL has dead-locks\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"
            - alert: PostgresqlTooManyConnections
              expr: sum by (instance, job, server) (pg_stat_activity_count) > min by (instance, job, server) (pg_settings_max_connections * 0.8)
              for: 2m
              labels:
                severity: warning
              annotations:
                summary: Postgresql too many connections (instance {{ $labels.instance }})
                description: "PostgreSQL instance has too many connections (> 80%).\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"
        
        - name: Nginx
          rules:
            - alert: NginxHighHttp4xxErrorRate
              expr: sum(rate(nginx_http_requests_total{status=~"^4.."}[1m])) / sum(rate(nginx_http_requests_total[1m])) * 100 > 5
              for: 1m
              labels:
                severity: critical
              annotations:
                summary: Nginx high HTTP 4xx error rate (instance {{ $labels.instance }})
                description: "Too many HTTP requests with status 4xx (> 5%)\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"
            - alert: NginxHighHttp5xxErrorRate
              expr: sum(rate(nginx_http_requests_total{status=~"^5.."}[1m])) / sum(rate(nginx_http_requests_total[1m])) * 100 > 5
              for: 1m
              labels:
                severity: critical
              annotations:
                summary: Nginx high HTTP 5xx error rate (instance {{ $labels.instance }})
                description: "Too many HTTP requests with status 5xx (> 5%)\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"
            - alert: NginxLatencyHigh
              expr: histogram_quantile(0.99, sum(rate(nginx_http_request_duration_seconds_bucket[2m])) by (host, node, le)) > 3
              for: 2m
              labels:
                severity: warning
              annotations:
                summary: Nginx latency high (instance {{ $labels.instance }})
                description: "Nginx p99 latency is higher than 3 seconds\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

        - name: Kubernetes
          rules:
            - alert: KubernetesNodeNotReady
              expr: kube_node_status_condition{condition="Ready",status="true"} == 0
              for: 10m
              labels:
                severity: critical
              annotations:
                summary: Kubernetes Node not ready (instance {{ $labels.instance }})
                description: "Node {{ $labels.node }} has been unready for a long time\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"
            - alert: KubernetesNodeMemoryPressure
              expr: kube_node_status_condition{condition="MemoryPressure",status="true"} == 1
              for: 2m
              labels:
                severity: critical
              annotations:
                summary: Kubernetes Node memory pressure (instance {{ $labels.instance }})
                description: "Node {{ $labels.node }} has MemoryPressure condition\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"
            - alert: KubernetesNodeDiskPressure
              expr: kube_node_status_condition{condition="DiskPressure",status="true"} == 1
              for: 2m
              labels:
                severity: critical
              annotations:
                summary: Kubernetes Node disk pressure (instance {{ $labels.instance }})
                description: "Node {{ $labels.node }} has DiskPressure condition\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"
            - alert: KubernetesNodeNetworkUnavailable
              expr: kube_node_status_condition{condition="NetworkUnavailable",status="true"} == 1
              for: 2m
              labels:
                severity: critical
              annotations:
                summary: Kubernetes Node network unavailable (instance {{ $labels.instance }})
                description: "Node {{ $labels.node }} has NetworkUnavailable condition\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"
            - alert: KubernetesNodeOutOfPodCapacity
              expr: sum by (node) ((kube_pod_status_phase{phase="Running"} == 1) + on(uid) group_left(node) (0 * kube_pod_info{pod_template_hash=""})) / sum by (node) (kube_node_status_allocatable{resource="pods"}) * 100 > 90
              for: 2m
              labels:
                severity: warning
              annotations:
                summary: Kubernetes Node out of pod capacity (instance {{ $labels.instance }})
                description: "Node {{ $labels.node }} is out of pod capacity\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"
            - alert: KubernetesContainerOomKiller
              expr: (kube_pod_container_status_restarts_total - kube_pod_container_status_restarts_total offset 10m >= 1) and ignoring (reason) min_over_time(kube_pod_container_status_last_terminated_reason{reason="OOMKilled"}[10m]) == 1
              for: 0m
              labels:
                severity: warning
              annotations:
                summary: Kubernetes Container oom killer (instance {{ $labels.instance }})
                description: "Container {{ $labels.container }} in pod {{ $labels.namespace }}/{{ $labels.pod }} has been OOMKilled {{ $value }} times in the last 10 minutes.\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"
            - alert: KubernetesVolumeFullInFourDays
              expr: predict_linear(kubelet_volume_stats_available_bytes[6h:5m], 4 * 24 * 3600) < 0
              for: 0m
              labels:
                severity: critical
              annotations:
                summary: Kubernetes Volume full in four days (instance {{ $labels.instance }})
                description: "Volume under {{ $labels.namespace }}/{{ $labels.persistentvolumeclaim }} is expected to fill up within four days. Currently {{ $value | humanize }}% is available.\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"
            - alert: KubernetesApiServerErrors
              expr: sum(rate(apiserver_request_total{job="apiserver",code=~"^(?:5..)$"}[1m])) / sum(rate(apiserver_request_total{job="apiserver"}[1m])) * 100 > 3
              for: 2m
              labels:
                severity: critical
              annotations:
                summary: Kubernetes API server errors (instance {{ $labels.instance }})
                description: "Kubernetes API server is experiencing high error rate\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"
            - alert: KubernetesClientCertificateExpiresNextWeek
              expr: apiserver_client_certificate_expiration_seconds_count{job="apiserver"} > 0 and histogram_quantile(0.01, sum by (job, le) (rate(apiserver_client_certificate_expiration_seconds_bucket{job="apiserver"}[5m]))) < 7*24*60*60
              for: 0m
              labels:
                severity: warning
              annotations:
                summary: Kubernetes client certificate expires next week (instance {{ $labels.instance }})
                description: "A client certificate used to authenticate to the apiserver is expiring next week.\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

        - name: Hashicorp Vault
          rules:
            - alert: VaultSealed
              expr: vault_core_unsealed == 0
              for: 0m
              labels:
                severity: critical
              annotations:
                summary: Vault sealed (instance {{ $labels.instance }})
                description: "Vault instance is sealed on {{ $labels.instance }}\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"
            - alert: VaultTooManyPendingTokens
              expr: avg(vault_token_create_count - vault_token_store_count) > 0
              for: 5m
              labels:
                severity: warning
              annotations:
                summary: Vault too many pending tokens (instance {{ $labels.instance }})
                description: "Too many pending tokens {{ $labels.instance }}: {{ $value | printf \"%.2f\"}}%\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"
            - alert: VaultTooManyInfinityTokens
              expr: vault_token_count_by_ttl{creation_ttl="+Inf"} > 3
              for: 5m
              labels:
                severity: warning
              annotations:
                summary: Vault too many infinity tokens (instance {{ $labels.instance }})
                description: "Too many infinity tokens {{ $labels.instance }}: {{ $value | printf \"%.2f\"}}%\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"
            - alert: VaultClusterHealth
              expr: sum(vault_core_active) / count(vault_core_active) <= 0.5
              for: 0m
              labels:
                severity: critical
              annotations:
                summary: Vault cluster health (instance {{ $labels.instance }})
                description: "Vault cluster is not healthy {{ $labels.instance }}: {{ $value | printf \"%.2f\"}}%\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

        - name: Pods
          rules:
            - alert: PodCrashLooping
              annotations:
                description: Pod {{ $labels.namespace }}/{{ $labels.pod }} ({{ $labels.container }}) is restarting {{ printf "%.2f" $value }} times / 5 minutes.
                summary: Pod is crash looping.
              expr: rate(kube_pod_container_status_restarts_total{job="kube-state-metrics",namespace=~".*"}[5m]) * 60 * 5 > 0
              for: 5m
              labels:
                severity: critical

  server:
    podAnnotations:
      vault.hashicorp.com/agent-inject: "true"
      vault.hashicorp.com/role: internal-app
      vault.hashicorp.com/agent-inject-template-hashicorp_vault_token: |
        {{- with secret "kv/data/secrets/hashicorp_vault" -}}
        {{- .Data.data.PROMETHEUS_TOKEN }}
        {{- end }}
    service:
      annotations:
        metallb.universe.tf/loadBalancerIPs: 10.10.10.206
      type: LoadBalancer
  extraScrapeConfigs: |
    - job_name: 'cert-manager'
      metrics_path: "/metrics"
      static_configs:
      - targets:
        - cert-manager.cert-manager.svc:9402

    - job_name: 'vault'
      metrics_path: "/v1/sys/metrics"
      scheme: http
      static_configs:
      - targets:
        - hashicorp-vault.security.svc:8200

    - job_name: proxmox-pve
      static_configs:
        - targets:
          - 192.168.1.7
      metrics_path: /pve
      params:
        module: [ default ]
        cluster: ['1']
        node: ['1']
      relabel_configs:
        - source_labels: [ __address__ ]
          target_label: __param_target
        - source_labels: [ __param_target ]
          target_label: instance
        - target_label: __address__
          replacement: pve-exporter:9221

    - job_name: raspberry-pi
      static_configs:
        - targets:
          - 192.168.6.27:9100
          - 192.168.6.180:9100
          - 192.168.6.216:9100
      metrics_path: /metrics

    - job_name: gpu-server
      static_configs:
        - targets:
          - 192.168.1.122:9100
          - 192.168.1.122:9400
      metrics_path: /metrics

prometheus-postgres-exporter:
  serviceMonitor:
    enabled: true